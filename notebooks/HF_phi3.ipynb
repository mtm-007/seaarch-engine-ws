{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b03c8c9-8944-4b1c-ac98-0ec8ed77c137",
   "metadata": {},
   "source": [
    "### chatcompletition, Prompt Template, LLM api call with Open Source LLMs,with local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d197dc6-d020-49e5-84e5-331aa09c9a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import minsearch\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973b6fdf-6210-4bd6-b23c-5c4a450d4135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7f2519cdbd00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)\n",
    "\n",
    "documents=[]\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)\n",
    "        \n",
    "Index = minsearch.Index(\n",
    "    text_fields = ['question','section','text'],\n",
    "    keyword_fields = ['course']\n",
    ")  \n",
    "\n",
    "Index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce318635-02e5-4ff5-b8b1-bb43c64dfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3, 'section' : 0.4}\n",
    "\n",
    "    results = Index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course':'mlops-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    ")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2012a964-1060-46ae-80f4-df245cf0ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\" \n",
    "You are an expert machine learning and mlops engineering helping a junior engineer as an assitant and guide. \n",
    "Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering. DO NOT USE OTHER CONTENT OTHER THAN GIVEN CONTEXT!\n",
    "if the CONTEXT does not contain the answer, Output \"Not FOUND in the context given\" and explain your answer with reasons.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: {context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7756d4e-ddfd-455b-8b46-838c2c416e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt):\n",
    "    client = Groq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "    return print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d5e338-dfc8-4130-9a8f-d0fc01bb9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = \"How to use mlflow for experiment tracking?\"\n",
    "def rag(query):\n",
    "    \n",
    "    search_results = search(query)\n",
    "    Prompt = build_prompt(query, search_results)\n",
    "    answer = llm_call(Prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481f53d-29f0-4cdd-9618-58e0547d65ce",
   "metadata": {},
   "source": [
    "### PHi3-Mini openSource LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57eb3c3-daed-42c5-a412-a1f952121014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f249970b9f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "torch.random.manual_seed(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3cdc1f-b91d-4f89-819b-e23b0495bf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da609145a2764392b62071caf98c2af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedcb0575f5c4c36b24a7114ac749ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b38fc774cb4a23a9cacbbc3aa75499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135639e67add42c5adbcda03ad1d4b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a48271c224407ea8b2b5ed7d642b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b38a62c3954b5cbefa18856a4c53ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b743afb1777a4195af27c138dfcb906d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700227c4f56941db9f5245c4457645a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2291deec06234262a6d0485c99158330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7def1b8096204ae4926213f827fdff26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02edea81a424f1fbb1b83dba81b7ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcbb43b36944c93a4e9a78086ac58aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acf43b531f64badb81a8149554e771a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d302a5600324443eb93271d4b58e8f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5059bbcc-0b82-4346-a0c7-45fa2f678f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953d090c-f4d7-4ff2-a5c0-623886984f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, you can still join the course. It's never too late to start learning something new. If you're interested in the course, I recommend checking the course details and prerequisites to ensure it aligns with your current knowledge and goals. If you have any specific questions or need guidance on how to enroll, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": \"Can I still join, I just descovered this course?.\"},     \n",
    "] \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563d2c8-6f82-4c13-866f-63f30933e209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a466627a-575b-49d4-a4d3-c5d8043f75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt):\n",
    "    messages = [ \n",
    "    {\"role\": \"system\", \"content\":prompt},     \n",
    "    ] \n",
    "\n",
    "    generation_args = { \n",
    "        \"max_new_tokens\": 500, \n",
    "        \"return_full_text\": False, \n",
    "        \"temperature\": 0.0, \n",
    "        \"do_sample\": False, \n",
    "    } \n",
    "    \n",
    "    output = pipe(messages, **generation_args) \n",
    "    return output[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e14da2d-2d48-43fe-8a3c-aa9a676ac3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To start using mlflow, follow these steps:\\n\\n1. Read the pins and bookmarks on the course-channel.\\n2. Read the repository (bookmarked in channel) and watch the video lessons (playlist bookmarked in channel).\\n3. If you have questions, search the channel itself first; someone may have already asked and gotten a solution.\\n4. If you don\\'t want to comb through the search results, read the FAQ document for the most Frequently Asked Questions (this document).\\n5. If you don\\'t even want to read/skim/search the questions in the FAQ doc, tag the @ZoomcampQABot when you ask questions, and it will summarize it for you (if answers in its knowledge-base).\\n6. For generic, non-zoomcamp queries, you can also ask ChatGPT/BingCopilot/GoogleGemini/etc, especially for error messages.\\n7. Check if you\\'re on track by checking the deadlines (in Course Management form for Homework submissions).\\n\\nIf you need to ask questions when no answers supplied in resources above have helped, follow the asking-questions.md (bookmarked in channel) guidelines, also in Pins...\\n\\nFor viewing MLflow Experiments using MLflow CLI, set the environment variable MLFLOW_TRACKING_URI to the URI of the sqlite database. For example, \"export MLFLOW_TRACKING_URI=sqlite:///{path to sqlite database}\". After this, you can view the experiments from the command line using commands like “mlflow experiments search”.\\n\\nWhen using Autologging, you don\\'t need to set a training parameter to track it on Mlflow UI. In the official documentation, it\\'s mentioned that autologging keeps track of the parameters even when you do not explicitly set them when calling.fit.\\n\\nIf you encounter a warning when using MLflow’s autolog function, the problem is that mlflow expects the dataset to be in a pd.DataFrame format, but if you’re following the course’s code, we’re providing a numpy.ndarray. To fix this, you can put log_datasets = False as a parameter in the autolog function.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"How do i start using mlflow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7095e-8a51-44fd-b0ce-f32d101d25f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9c823-de9b-4493-8379-c879e2232d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644d53d-b790-48ae-9a89-e77873ad7589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b258b-9699-45cb-bbed-1bd408f12b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
