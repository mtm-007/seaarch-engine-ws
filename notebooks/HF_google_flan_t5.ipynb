{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b03c8c9-8944-4b1c-ac98-0ec8ed77c137",
   "metadata": {},
   "source": [
    "### chatcompletition, Prompt Template, LLM api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d197dc6-d020-49e5-84e5-331aa09c9a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import minsearch\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973b6fdf-6210-4bd6-b23c-5c4a450d4135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7ff53ecb1ab0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)\n",
    "\n",
    "documents=[]\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)\n",
    "        \n",
    "Index = minsearch.Index(\n",
    "    text_fields = ['question','section','text'],\n",
    "    keyword_fields = ['course']\n",
    ")  \n",
    "\n",
    "Index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce318635-02e5-4ff5-b8b1-bb43c64dfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3, 'section' : 0.4}\n",
    "\n",
    "    results = Index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course':'mlops-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    ")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2012a964-1060-46ae-80f4-df245cf0ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\" \n",
    "You are an expert machine learning and mlops engineering helping a junior engineer as an assitant and guide. \n",
    "Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering. DO NOT USE OTHER CONTENT OTHER THAN GIVEN CONTEXT!\n",
    "if the CONTEXT does not contain the answer, Output \"Not FOUND in the context given\" and explain your answer with reasons.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: {context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7756d4e-ddfd-455b-8b46-838c2c416e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt):\n",
    "    client = Groq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "    return print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d5e338-dfc8-4130-9a8f-d0fc01bb9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = \"How to use mlflow for experiment tracking?\"\n",
    "def rag(query):\n",
    "    \n",
    "    search_results = search(query)\n",
    "    Prompt = build_prompt(query, search_results)\n",
    "    answer = llm_call(Prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a341f1b-54f0-462d-98e3-8c5f0781b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6629a97-668f-40b1-8b91-7bc1999bfaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da514cd175d48aa840711a28506d35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cd5a9eb-9125-4e39-bf46-6df4d0d97b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(input_ids)\n",
    "result = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "260fd4cc-db62-425a-806f-dd2498a5c897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Wie alt sind Sie?</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a466627a-575b-49d4-a4d3-c5d8043f75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt):\n",
    "    #this call is going to produce max length error\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids)\n",
    "    result = tokenizer.decode(outputs[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e0a2f89-001d-4a6e-acfe-81fa933e88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = \"How to register for the course, I started late?\"\n",
    "def rag(query):\n",
    "    \n",
    "    search_results = search(query)\n",
    "    Prompt = build_prompt(query, search_results)\n",
    "    answer = llm_call(Prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7734452-fd62-4875-aa76-b61783a85060",
   "metadata": {},
   "source": [
    "#### max size limit example error msg, solution hits sliding window\n",
    "- Token indices sequence length is longer than the specified maximum sequence length for this model (1472 > 512). Running this sequence through the model will result in indexing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b1f7aa1-deed-4d72-b2f3-7f5f8e106784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt, max_length=512, min_length=100, temperature=0.7, top_k=50, top_p=0.95, num_beams=5):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,  # Set to True to enable sampling and allow for varied responses\n",
    "        num_beams=num_beams  # Number of beams for beam search\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ba43f3c-9368-4fd0-b0b9-f42079a8a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You don't need to register, registration is not mandatory. It is for gauging the level of interest and collecting data for analytics. You can also just start learning and submitting homework without registering while a cohort is “live”. Registration is just to gauge interest before the start date. section: General course questions question: Course - How do I start? answer: See what things are where by reading pins and bookmarks on the course-channel reading the repo (bookmarked in channel) and watching the video lessons (playlist bookmarked\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55461b47-1686-4e75-8c77-12bd200eb485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644791c6-6664-4a88-aa56-5326f9d322be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650cc41b-d6d5-4bcb-80cf-15c3ada2baa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
